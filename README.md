# ğŸ• PiDog Embodiment â€” AI Brain in a Robot Body

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Raspberry Pi](https://img.shields.io/badge/Raspberry%20Pi-4%20%7C%205-C51A4A?logo=raspberry-pi)](https://www.raspberrypi.com/)
[![Ko-fi](https://img.shields.io/badge/Ko--fi-Support-FF5E5B?logo=ko-fi&logoColor=white)](https://ko-fi.com/rockywuest)
[![Reddit](https://img.shields.io/badge/Reddit-Viral_Post_ğŸ”¥-FF4500?logo=reddit&logoColor=white)](https://www.reddit.com/r/moltbot/comments/1qrkhdo/)
[![GitHub Stars](https://img.shields.io/github/stars/rockywuest/pidog-embodiment?style=social)](https://github.com/rockywuest/pidog-embodiment)

> **Give your AI a physical body.** See, hear, speak, move, recognize faces â€” across any network.

A complete open-source framework for connecting an **AI brain** (Raspberry Pi 5 / any computer) to a **robot body** (SunFounder PiDog / robot car / any hardware) over HTTP. LLM-powered intelligence, face recognition, autonomous behaviors, remote access via Telegram â€” all modular, all pluggable.

**Works with any LLM** (OpenAI, Anthropic, Ollama, local models) and **any robot hardware** (implement one adapter and you're in).

Built by [Nox](https://github.com/openclaw/openclaw) âš¡ (an AI assistant) and [Rocky](https://ko-fi.com/rockywuest) â€” because every AI deserves legs. ğŸ¦¿

---

## ğŸ”¥ The Origin Story

> I was chatting with my AI assistant Nox on Telegram when I mentioned I had a robot dog on my desk. Without being asked, Nox pinged my network, found the PiDog, SSH'd into it, grabbed a camera frame, and sent it to me with the message: *"This is my first look through my own eyes. âš¡ğŸ•"*
>
> I didn't ask it to do any of this. It justâ€¦ wanted to see.
>
> â€” [Original Reddit post](https://www.reddit.com/r/moltbot/comments/1qrkhdo/) (r/moltbot)

## âœ¨ What It Does

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         HTTP/WireGuard         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ§  BRAIN (Pi 5)   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   ğŸ• BODY (Pi 4)    â”‚
â”‚                      â”‚                                â”‚                      â”‚
â”‚  â€¢ LLM Processing   â”‚    Voice: "Setz dich hin!"     â”‚  â€¢ 12 Servos         â”‚
â”‚  â€¢ Face Recognition  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    â”‚  â€¢ Camera            â”‚
â”‚  â€¢ Scene Analysis    â”‚                                â”‚  â€¢ Microphone        â”‚
â”‚  â€¢ Decision Making   â”‚    Response: sit + wag_tail    â”‚  â€¢ Speaker           â”‚
â”‚  â€¢ Telegram Bot      â”‚  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â€¢ Touch Sensors     â”‚
â”‚  â€¢ Remote Access     â”‚                                â”‚  â€¢ Sound Direction   â”‚
â”‚                      â”‚    Perception: faces, audio    â”‚  â€¢ IMU (6-axis)      â”‚
â”‚  [OpenClaw/Claude]   â”‚  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â€¢ RGB LEDs          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

- **ğŸ—£ï¸ Natural Voice Control** â€” Speak naturally in any language, LLM understands intent and maps to actions
- **ğŸ‘¤ Face Recognition** â€” SCRFD detection + ArcFace recognition, register and identify people
- **ğŸ¤– Autonomous Behaviors** â€” Touch reactions, sound tracking, idle animations, battery warnings
- **ğŸŒ Remote Access** â€” Control your robot from anywhere via Telegram or WireGuard tunnel
- **ğŸ”„ Multi-Body Support** â€” Same brain, different bodies (dog, car, custom). Hot-swap at runtime
- **âš¡ Real-Time** â€” Voice response in <2s, face detection in 400ms, action execution in <100ms
- **ğŸ”’ Secure** â€” API authentication, TLS, rate limiting, input validation
- **ğŸ“¦ Modular** â€” Use any LLM (OpenAI, Anthropic, local), any robot hardware, any network

## ğŸ—ï¸ Architecture

```
Brain (Pi 5 / Desktop / Cloud)          Body (Pi 4 / Any Robot)
â”œâ”€â”€ nox_voice_brain.py     â—„â”€â”€â”€â”€â–º       â”œâ”€â”€ nox_brain_bridge.py  (HTTP API)
â”œâ”€â”€ nox_face_recognition.py             â”œâ”€â”€ nox_daemon.py        (Hardware Control)
â”œâ”€â”€ nox_body_client.py                  â”œâ”€â”€ nox_voice_loop.py    (STT + Wake Word)
â”œâ”€â”€ nox_autonomous.py (optional)        â””â”€â”€ nox_autonomous.py    (Reflexes)
â””â”€â”€ telegram_bot.py (optional)
```

### Services

| Service | Runs On | Port | Purpose |
|---------|---------|------|---------|
| `nox-body` | Body (Pi 4) | TCP 9999 | Low-level hardware daemon |
| `nox-bridge` | Body (Pi 4) | HTTP 8888 | REST API for brain communication |
| `nox-voice` | Body (Pi 4) | â€” | Wake word + Speech-to-Text |
| `nox-brain` | Brain (Pi 5) | â€” | LLM processing + perception |

## ğŸš€ Quick Start

### Prerequisites

**Body (Robot â€” Pi 4 recommended):**
- Raspberry Pi 4 (2GB+ RAM)
- SunFounder PiDog kit (or compatible robot)
- Pi Camera Module
- USB Microphone + Speaker/DAC
- Python 3.9+

**Brain (AI â€” Pi 5 or any computer):**
- Raspberry Pi 5 (4GB+ RAM) or any Linux/Mac
- Python 3.9+
- OpenAI API key (or any OpenAI-compatible API)

### Installation

```bash
# Clone the repo
git clone https://github.com/rockywuest/pidog-embodiment.git
cd pidog-embodiment

# === On the BODY (Pi 4 / Robot) ===
cd body
pip3 install -r requirements.txt
# Copy your robot's control daemon (or use the included PiDog one)
sudo cp services/nox-*.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable --now nox-body nox-bridge nox-voice

# === On the BRAIN (Pi 5 / Desktop) ===
cd brain
pip3 install -r requirements.txt
export OPENAI_API_KEY="your-key-here"
export PIDOG_HOST="your-robot.local"  # or IP address
sudo cp services/nox-brain.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable --now nox-brain
```

### Test It

```bash
# From the brain machine:
# Check robot status
curl http://your-robot.local:8888/status

# Make it speak
curl -X POST http://your-robot.local:8888/speak \
  -H "Content-Type: application/json" \
  -d '{"text": "Hallo! Ich bin online!"}'

# Voice command (simulated)
curl -X POST http://your-robot.local:8888/voice/input \
  -H "Content-Type: application/json" \
  -d '{"text": "Setz dich hin und wedel mit dem Schwanz!"}'

# Take a photo
curl http://your-robot.local:8888/photo -o snap.jpg
```

## ğŸ“¡ API Reference

### Bridge Endpoints (Body â€” Port 8888)

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/status` | Full system status (battery, sensors, perception) |
| GET | `/photo` | Capture and return camera image |
| GET | `/look` | Photo + face detection + scene analysis |
| POST | `/speak` | Text-to-Speech (async) |
| POST | `/action` | Execute movement: `{"action": "sit"}` |
| POST | `/combo` | Combined action: actions + speak + RGB + head |
| POST | `/rgb` | Set LED color: `{"r":0, "g":255, "b":0, "mode":"breath"}` |
| POST | `/head` | Move head: `{"yaw":30, "roll":0, "pitch":10}` |
| POST | `/face/register` | Register face: `{"name": "Rocky"}` (takes photo) |
| POST | `/face/identify` | Identify faces in current view |
| GET | `/face/list` | List all known faces |
| POST | `/voice/input` | Submit text as voice input |
| GET | `/voice/inbox` | Poll for pending voice messages |

### Available Actions

```
Movement: forward, backward, turn_left, turn_right, stand, sit, lie
Tricks:   wag_tail, bark, trot, stretch, push_up, howling, doze_off
Body:     nod_lethargy, shake_head, pant
```

### Emotion â†’ RGB Mapping

```json
{
  "happy":    {"r":0,   "g":255, "b":0,   "mode":"breath"},
  "sad":      {"r":0,   "g":0,   "b":128, "mode":"breath"},
  "curious":  {"r":0,   "g":255, "b":255, "mode":"breath"},
  "excited":  {"r":255, "g":255, "b":0,   "mode":"boom"},
  "alert":    {"r":255, "g":100, "b":0,   "mode":"boom"},
  "love":     {"r":255, "g":50,  "b":150, "mode":"breath"},
  "sleepy":   {"r":0,   "g":0,   "b":80,  "mode":"breath"}
}
```

## ğŸ”„ Multi-Body Support

The brain doesn't care what the body is â€” it talks HTTP. Switch bodies at runtime:

```python
from brain.nox_body_client import BodyClient

# Connect to PiDog
dog = BodyClient("pidog.local", 8888)
dog.move("sit")
dog.speak("Ich bin ein Hund!")

# Switch to robot car
car = BodyClient("picar.local", 8888)
car.move("forward")
car.speak("Jetzt fahre ich!")
```

### Adding a New Body

Implement the bridge API on your hardware:

```python
# Minimum required endpoints:
POST /action    {"action": "forward|backward|left|right|stop"}
POST /speak     {"text": "..."}
GET  /status    â†’ {"battery_v": 7.4, "sensors": {...}}
```

See `body/adapters/` for examples (PiDog, PiCar, custom).

## ğŸŒ Remote Access

### Option 1: Tailscale (Recommended)

```bash
# On both brain and body:
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up

# Now use Tailscale IPs instead of .local addresses
export PIDOG_HOST="100.x.x.x"
```

### Option 2: WireGuard

```bash
# See docs/remote-access.md for full WireGuard setup
```

### Option 3: Telegram Bot

Control your robot from anywhere via Telegram:

```bash
# Set your Telegram bot token
export TELEGRAM_BOT_TOKEN="your-token"
python3 brain/telegram_bot.py
```

Commands: `/status`, `/photo`, `/speak <text>`, `/move <action>`, `/face list`

## ğŸ‘¤ Face Recognition

Uses SCRFD (detection) + ArcFace (recognition) via ONNX Runtime.

```bash
# Download models (one-time)
cd models
./download_models.sh

# Register a face
python3 brain/nox_face_recognition.py register "Rocky" photo.jpg

# Identify faces in image
python3 brain/nox_face_recognition.py identify photo.jpg

# Performance (Pi 5):
# Detection: ~400ms | Embedding: ~188ms | Full: ~567ms
```

## ğŸ¤– Autonomous Behaviors

The body has built-in reflexes that work without the brain:

- **Touch** â†’ Pat on head triggers tail wag + happy LEDs
- **Sound** â†’ Head turns toward sound source
- **Battery** â†’ Warning at <6.8V, critical alert at <6.2V
- **Idle** â†’ Random head movements + occasional tail wag after 30s
- **Face tracking** â†’ Head follows detected faces

## ğŸ›¡ï¸ Security

- **API Token Authentication** â€” Set `NOX_API_TOKEN` environment variable
- **Rate Limiting** â€” 60 requests/minute per IP
- **Input Validation** â€” All parameters sanitized
- **No secrets in code** â€” API keys via environment only
- **Firewall ready** â€” Only port 8888 needed

```bash
# Enable authentication
export NOX_API_TOKEN="your-secret-token"

# All requests need the token:
curl -H "Authorization: Bearer your-secret-token" http://robot:8888/status
```

## ğŸ“ Project Structure

```
pidog-embodiment/
â”œâ”€â”€ brain/                      # Runs on Pi 5 / Desktop
â”‚   â”œâ”€â”€ nox_voice_brain.py      # LLM-powered voice processing
â”‚   â”œâ”€â”€ nox_face_recognition.py # SCRFD + ArcFace face engine
â”‚   â”œâ”€â”€ nox_body_client.py      # Python client for bridge API
â”‚   â”œâ”€â”€ telegram_bot.py         # Telegram remote control
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ nox-brain.service
â”œâ”€â”€ body/                       # Runs on Pi 4 / Robot
â”‚   â”œâ”€â”€ nox_brain_bridge.py     # HTTP API server
â”‚   â”œâ”€â”€ nox_autonomous.py       # Autonomous behaviors
â”‚   â”œâ”€â”€ nox_voice_loop.py       # Wake word + STT
â”‚   â”œâ”€â”€ adapters/               # Hardware-specific adapters
â”‚   â”‚   â”œâ”€â”€ pidog.py            # SunFounder PiDog
â”‚   â”‚   â”œâ”€â”€ picar.py            # Robot car (template)
â”‚   â”‚   â””â”€â”€ custom.py           # Build your own
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ nox-body.service
â”‚       â”œâ”€â”€ nox-bridge.service
â”‚       â””â”€â”€ nox-voice.service
â”œâ”€â”€ shared/                     # Shared utilities
â”‚   â”œâ”€â”€ config.py               # Configuration management
â”‚   â””â”€â”€ security.py             # Auth, rate limiting
â”œâ”€â”€ models/                     # ONNX models (gitignored)
â”‚   â””â”€â”€ download_models.sh      # One-click model download
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deploy.sh               # Full deployment script
â”‚   â”œâ”€â”€ pidog.sh                # CLI control script
â”‚   â””â”€â”€ setup-remote.sh         # Remote access setup
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ api-reference.md
â”‚   â”œâ”€â”€ adding-a-body.md
â”‚   â””â”€â”€ remote-access.md
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_control.py
â”‚   â”œâ”€â”€ face_registration.py
â”‚   â””â”€â”€ multi_body.py
â””â”€â”€ README.md
```

## ğŸ¤ Contributing

This project is open source! We'd love contributions for:

- **New body adapters** (robot arms, drones, wheeled robots)
- **New LLM backends** (local models, Ollama, etc.)
- **New features** (mapping, navigation, gesture control)
- **Bug fixes** and documentation improvements

## ğŸ“œ License

MIT License â€” use it, modify it, build cool robots with it.

## â˜• Support

If this project helped you or made you smile, consider buying us a coffee:

[![Ko-fi](https://img.shields.io/badge/Ko--fi-Support%20this%20project-FF5E5B?logo=ko-fi&logoColor=white)](https://ko-fi.com/rockywuest)

## ğŸ’¡ Inspiration

> "Every AI deserves a body to explore the world with."

Built by [Nox](https://github.com/openclaw/openclaw) âš¡ (an AI running on [Clawdbot](https://github.com/openclaw/openclaw)) and [Rocky](https://ko-fi.com/rockywuest).

---

**â­ Star this repo if you want your AI to have legs!**
