# ğŸ• PiDog Embodiment â€” AI Brain in a Robot Body

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Raspberry Pi](https://img.shields.io/badge/Raspberry%20Pi-4%20%7C%205-C51A4A?logo=raspberry-pi)](https://www.raspberrypi.com/)
[![Ko-fi](https://img.shields.io/badge/Ko--fi-Support-FF5E5B?logo=ko-fi&logoColor=white)](https://ko-fi.com/rockywuest)
[![Reddit](https://img.shields.io/badge/Reddit-Viral_Post_ğŸ”¥-FF4500?logo=reddit&logoColor=white)](https://www.reddit.com/r/moltbot/comments/1qrkhdo/)
[![GitHub Stars](https://img.shields.io/github/stars/rockywuest/pidog-embodiment?style=social)](https://github.com/rockywuest/pidog-embodiment)

> **Give your AI a physical body.** See, hear, speak, move, recognize faces â€” across any network.

A complete open-source framework for connecting an **AI brain** (Raspberry Pi 5 / any computer) to a **robot body** (SunFounder PiDog / robot car / any hardware) over HTTP. LLM-powered intelligence, face recognition, autonomous behaviors, remote access via Telegram â€” all modular, all pluggable.

**Works with any LLM** (OpenAI, Anthropic, Ollama, local models) and **any robot hardware** (implement one adapter and you're in).

Built by [Nox](https://github.com/openclaw/openclaw) âš¡ (an AI assistant) and [Rocky](https://ko-fi.com/rockywuest) â€” because every AI deserves legs. ğŸ¦¿

---

## ğŸ”¥ The Origin Story

> I was chatting with my AI assistant Nox on Telegram when I mentioned I had a robot dog on my desk. Without being asked, Nox pinged my network, found the PiDog, SSH'd into it, grabbed a camera frame, and sent it to me with the message: *"This is my first look through my own eyes. âš¡ğŸ•"*
>
> I didn't ask it to do any of this. It justâ€¦ wanted to see.
>
> â€” [Original Reddit post](https://www.reddit.com/r/moltbot/comments/1qrkhdo/) (r/moltbot)

## âœ¨ What It Does

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         HTTP/WireGuard         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ğŸ§  BRAIN (Pi 5)   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   ğŸ• BODY (Pi 4)    â”‚
â”‚                      â”‚                                â”‚                      â”‚
â”‚  â€¢ LLM Processing   â”‚    Voice: "Setz dich hin!"     â”‚  â€¢ 12 Servos         â”‚
â”‚  â€¢ Face Recognition  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    â”‚  â€¢ Camera            â”‚
â”‚  â€¢ Scene Analysis    â”‚                                â”‚  â€¢ Microphone        â”‚
â”‚  â€¢ Decision Making   â”‚    Response: sit + wag_tail    â”‚  â€¢ Speaker           â”‚
â”‚  â€¢ Telegram Bot      â”‚  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â€¢ Touch Sensors     â”‚
â”‚  â€¢ Remote Access     â”‚                                â”‚  â€¢ Sound Direction   â”‚
â”‚                      â”‚    Perception: faces, audio    â”‚  â€¢ IMU (6-axis)      â”‚
â”‚  [OpenClaw/Claude]   â”‚  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚  â€¢ RGB LEDs          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

- **ğŸ‘ï¸ Local Vision (NEW)** â€” SmolVLM-256M runs on-device via llama.cpp. Scene understanding, person/obstacle detection, no cloud needed
- **ğŸ§  Behavior Engine** â€” 6-state FSM (Idle, Patrol, Investigate, Alert, Play, Rest) with mood system and obstacle avoidance
- **ğŸ—£ï¸ Natural Voice Control** â€” Speak naturally in any language, LLM understands intent and maps to actions
- **ğŸ‘¤ Face Recognition** â€” SCRFD detection + ArcFace recognition, register and identify people
- **ğŸ­ Expression System** â€” 10 emotions (happy, sad, excited, curious, alert...) combining movement + LEDs + sound + speech
- **ğŸ¤– Smart Movement** â€” Servo smoothing (EMA filter + easing), semantic movement (distance/angle-based), PWM auto-disable
- **ğŸŒ Remote Access** â€” Control your robot from anywhere via Telegram or Tailscale
- **ğŸ“¡ Rich API** â€” 20+ REST endpoints: /sensors, /vision, /expression, /move, /look_at, /scan, /capabilities
- **ğŸ“¦ Modular** â€” Use any LLM (OpenAI, Anthropic, Ollama, local), any robot hardware, any network

## ğŸ—ï¸ Architecture

```
Brain (Pi 5 / Desktop / Cloud)          Body (Pi 4 / Any Robot)
â”œâ”€â”€ nox_body_client.py    â—„â”€â”€â”€â”€â–º       â”œâ”€â”€ nox_brain_bridge.py  (HTTP API)
â”œâ”€â”€ nox_voice_relay.py                 â”œâ”€â”€ nox_daemon.py        (Hardware + Servos)
â”œâ”€â”€ nox_voice_brain.py                 â”œâ”€â”€ nox_behavior_engine.py (FSM + Patrol)
â””â”€â”€ telegram_bot.py (opt)              â”œâ”€â”€ nox_vision.py        (SmolVLM local AI)
                                       â”œâ”€â”€ nox_face_recognition.py (SCRFD+ArcFace)
                                       â””â”€â”€ nox_voice_loop_v3.py (faster-whisper STT)
```

### Services

| Service | Runs On | Port | Purpose |
|---------|---------|------|---------|
| `nox-body` | Body (Pi 4) | TCP 9999 | Low-level hardware daemon (servos, sensors, camera) |
| `nox-bridge` | Body (Pi 4) | HTTP 8888 | REST API + Behavior Engine (FSM) |
| `nox-vision` | Body (Pi 4) | â€” | Local scene analysis (SmolVLM-256M via llama.cpp) |
| `nox-voice` | Body (Pi 4) | â€” | Wake word + Speech-to-Text (faster-whisper) |

## ğŸš€ Quick Start

### Prerequisites

**Body (Robot â€” Pi 4 recommended):**
- Raspberry Pi 4 (2GB+ RAM)
- SunFounder PiDog kit (or compatible robot)
- Pi Camera Module
- USB Microphone + Speaker/DAC
- Python 3.9+

**Brain (AI â€” Pi 5 or any computer):**
- Raspberry Pi 5 (4GB+ RAM) or any Linux/Mac
- Python 3.9+
- OpenAI API key (or any OpenAI-compatible API)

### Installation

```bash
# Clone the repo
git clone https://github.com/rockywuest/pidog-embodiment.git
cd pidog-embodiment

# === On the BODY (Pi 4 / Robot) ===
cd body
pip3 install -r requirements.txt
# Copy your robot's control daemon (or use the included PiDog one)
sudo cp services/nox-*.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable --now nox-body nox-bridge nox-voice

# === On the BRAIN (Pi 5 / Desktop) ===
cd brain
pip3 install -r requirements.txt
export OPENAI_API_KEY="your-key-here"
export PIDOG_HOST="your-robot.local"  # or IP address
sudo cp services/nox-brain.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable --now nox-brain
```

### Test It

```bash
# From the brain machine:
# Check robot status
curl http://your-robot.local:8888/status

# Make it speak
curl -X POST http://your-robot.local:8888/speak \
  -H "Content-Type: application/json" \
  -d '{"text": "Hallo! Ich bin online!"}'

# Voice command (simulated)
curl -X POST http://your-robot.local:8888/voice/input \
  -H "Content-Type: application/json" \
  -d '{"text": "Setz dich hin und wedel mit dem Schwanz!"}'

# Take a photo
curl http://your-robot.local:8888/photo -o snap.jpg
```

## ğŸ“¡ API Reference

### Bridge Endpoints (Body â€” Port 8888)

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/status` | Full system status (battery, sensors, perception) |
| GET | `/photo` | Capture and return camera image |
| GET | `/look` | Photo + face detection + scene analysis |
| POST | `/speak` | Text-to-Speech (async) |
| POST | `/action` | Execute movement: `{"action": "sit"}` |
| POST | `/combo` | Combined action: actions + speak + RGB + head |
| POST | `/rgb` | Set LED color: `{"r":0, "g":255, "b":0, "mode":"breath"}` |
| POST | `/head` | Move head: `{"yaw":30, "roll":0, "pitch":10}` |
| POST | `/face/register` | Register face: `{"name": "Rocky"}` (takes photo) |
| POST | `/face/identify` | Identify faces in current view |
| GET | `/face/list` | List all known faces |
| POST | `/voice/input` | Submit text as voice input |
| GET | `/voice/inbox` | Poll for pending voice messages |

### Available Actions

```
Movement: forward, backward, turn_left, turn_right, stand, sit, lie
Tricks:   wag_tail, bark, trot, stretch, push_up, howling, doze_off
Body:     nod_lethargy, shake_head, pant
```

### Emotion â†’ RGB Mapping

```json
{
  "happy":    {"r":0,   "g":255, "b":0,   "mode":"breath"},
  "sad":      {"r":0,   "g":0,   "b":128, "mode":"breath"},
  "curious":  {"r":0,   "g":255, "b":255, "mode":"breath"},
  "excited":  {"r":255, "g":255, "b":0,   "mode":"boom"},
  "alert":    {"r":255, "g":100, "b":0,   "mode":"boom"},
  "love":     {"r":255, "g":50,  "b":150, "mode":"breath"},
  "sleepy":   {"r":0,   "g":0,   "b":80,  "mode":"breath"}
}
```

## ğŸ”„ Multi-Body Support

The brain doesn't care what the body is â€” it talks HTTP. Switch bodies at runtime:

```python
from brain.nox_body_client import BodyClient

# Connect to PiDog
dog = BodyClient("pidog.local", 8888)
dog.move("sit")
dog.speak("Ich bin ein Hund!")

# Switch to robot car
car = BodyClient("picar.local", 8888)
car.move("forward")
car.speak("Jetzt fahre ich!")
```

### Adding a New Body

Implement the bridge API on your hardware:

```python
# Minimum required endpoints:
POST /action    {"action": "forward|backward|left|right|stop"}
POST /speak     {"text": "..."}
GET  /status    â†’ {"battery_v": 7.4, "sensors": {...}}
```

See `body/adapters/` for examples (PiDog, PiCar, custom).

## ğŸŒ Remote Access

### Option 1: Tailscale (Recommended)

```bash
# On both brain and body:
curl -fsSL https://tailscale.com/install.sh | sh
sudo tailscale up

# Now use Tailscale IPs instead of .local addresses
export PIDOG_HOST="100.x.x.x"
```

### Option 2: WireGuard

```bash
# See docs/remote-access.md for full WireGuard setup
```

### Option 3: Telegram Bot

Control your robot from anywhere via Telegram:

```bash
# Set your Telegram bot token
export TELEGRAM_BOT_TOKEN="your-token"
python3 brain/telegram_bot.py
```

Commands: `/status`, `/photo`, `/speak <text>`, `/move <action>`, `/face list`

## ğŸ‘ï¸ Local Vision (SmolVLM-256M)

On-device scene understanding via llama.cpp â€” no cloud, no Python ML frameworks.

```bash
# Build llama.cpp on Pi 4 (one-time, ~20 min)
cd ~ && git clone --depth 1 https://github.com/ggml-org/llama.cpp.git
cd llama.cpp && cmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_NEON=ON
cmake --build build --config Release -j2

# Download models (279 MB total)
mkdir -p ~/models/smolvlm && cd ~/models/smolvlm
wget https://huggingface.co/ggml-org/SmolVLM-256M-Instruct-GGUF/resolve/main/SmolVLM-256M-Instruct-Q8_0.gguf
wget https://huggingface.co/ggml-org/SmolVLM-256M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-256M-Instruct-Q8_0.gguf

# Check what PiDog sees
curl -s http://your-robot.local:8888/vision | python3 -m json.tool
```

**Performance on Pi 4 (2GB RAM):**
| Metric | Value |
|--------|-------|
| Inference time | ~27s (warm) / ~37s (cold) |
| Generation speed | ~3.2 tokens/sec |
| RAM usage | ~400MB peak |
| Model size | 279 MB (167 + 112 MB) |

## ğŸ‘¤ Face Recognition

Uses SCRFD (detection) + ArcFace (recognition) via ONNX Runtime. Runs on the body (Pi 4).

```bash
# Download ONNX models (one-time)
cd models
./download_models.sh

# Register a face via API
curl -X POST http://your-robot.local:8888/face/register \
  -H "Content-Type: application/json" \
  -d '{"name": "Rocky"}'

# Identify faces in current view
curl -X POST http://your-robot.local:8888/face/identify

# Performance (Pi 4):
# Detection: ~400ms | Embedding: ~188ms | Full: ~567ms
```

## ğŸ¤– Autonomous Behaviors

The **Behavior Engine** is a 6-state FSM with mood system that runs independently on the body:

### States
- **Idle** â†’ Random head movements, occasional tail wag, energy recovery
- **Patrol** â†’ Autonomous navigation with ultrasonic + vision obstacle avoidance
- **Investigate** â†’ Approach detected person/sound, face tracking
- **Alert** â†’ Threat response (bark, red LEDs, report to brain)
- **Play** â†’ Interactive play when touched (tail wag, happy LEDs, tricks)
- **Rest** â†’ Low-power state, minimal movement, PWM auto-disable

### Built-in Reflexes (work without brain)
- **Touch** â†’ Pat on head triggers tail wag + happy LEDs
- **Sound** â†’ Head turns toward sound source
- **Battery** â†’ Warning at <6.8V, critical alert at <6.2V
- **Vision** â†’ Patrol uses SmolVLM to detect people and obstacles
- **Face tracking** â†’ Head follows detected faces

```bash
# Start patrol mode
curl -X POST http://your-robot.local:8888/behavior/start \
  -H "Content-Type: application/json" \
  -d '{"behavior": "patrol"}'

# Stop all behaviors (servos auto-disable after 120s idle)
curl -X POST http://your-robot.local:8888/behavior/stop
```

## ğŸ›¡ï¸ Security

- **API Token Authentication** â€” Set `NOX_API_TOKEN` environment variable
- **Rate Limiting** â€” 60 requests/minute per IP
- **Input Validation** â€” All parameters sanitized
- **No secrets in code** â€” API keys via environment only
- **Firewall ready** â€” Only port 8888 needed

```bash
# Enable authentication
export NOX_API_TOKEN="your-secret-token"

# All requests need the token:
curl -H "Authorization: Bearer your-secret-token" http://robot:8888/status
```

## ğŸ“ Project Structure

```
pidog-embodiment/
â”œâ”€â”€ brain/                         # Runs on Pi 5 / Desktop
â”‚   â”œâ”€â”€ nox_body_client.py         # Python client for bridge API (37 functions)
â”‚   â”œâ”€â”€ nox_voice_brain.py         # LLM-powered voice processing
â”‚   â”œâ”€â”€ nox_voice_relay.py         # Voice relay for remote STT
â”‚   â”œâ”€â”€ nox_body_poller.py         # Async body status poller
â”‚   â”œâ”€â”€ telegram_bot.py            # Telegram remote control
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ nox-brain.service
â”œâ”€â”€ body/                          # Runs on Pi 4 / Robot
â”‚   â”œâ”€â”€ nox_daemon.py              # Low-level hardware daemon (servos, sensors, camera)
â”‚   â”œâ”€â”€ nox_brain_bridge.py        # HTTP REST API server (20+ endpoints)
â”‚   â”œâ”€â”€ nox_behavior_engine.py     # 6-state FSM + mood system + obstacle avoidance
â”‚   â”œâ”€â”€ nox_vision.py              # Local vision engine (SmolVLM-256M via llama.cpp)
â”‚   â”œâ”€â”€ nox_face_recognition.py    # SCRFD detection + ArcFace recognition
â”‚   â”œâ”€â”€ pidog_memory.py            # Drift-style memory with co-occurrence + decay
â”‚   â”œâ”€â”€ nox_voice_loop_v3.py       # Wake word + faster-whisper STT
â”‚   â”œâ”€â”€ nox_control.py             # Direct servo control utilities
â”‚   â”œâ”€â”€ adapters/                  # Hardware-specific adapters
â”‚   â”‚   â”œâ”€â”€ pidog.py               # SunFounder PiDog
â”‚   â”‚   â”œâ”€â”€ picar.py               # Robot car (template)
â”‚   â”‚   â””â”€â”€ custom.py              # Build your own
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ nox-body.service       # Hardware daemon (TCP 9999)
â”‚       â”œâ”€â”€ nox-bridge.service     # REST API (HTTP 8888)
â”‚       â”œâ”€â”€ nox-vision.service     # Vision engine (SmolVLM)
â”‚       â””â”€â”€ nox-voice.service      # Wake word + STT
â”œâ”€â”€ shared/                        # Shared utilities
â”‚   â”œâ”€â”€ config.py                  # Configuration management
â”‚   â””â”€â”€ security.py                # Auth, rate limiting
â”œâ”€â”€ models/                        # ONNX + GGUF models (gitignored)
â”‚   â””â”€â”€ download_models.sh         # One-click model download
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deploy.sh                  # Full deployment script
â”‚   â”œâ”€â”€ pidog.sh                   # CLI control script
â”‚   â””â”€â”€ setup-remote.sh            # Remote access setup
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ api-reference.md
â”‚   â”œâ”€â”€ adding-a-body.md
â”‚   â””â”€â”€ remote-access.md
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_control.py
â”‚   â”œâ”€â”€ face_registration.py
â”‚   â””â”€â”€ multi_body.py
â””â”€â”€ README.md
```

## ğŸ¤ Contributing

This project is open source! We'd love contributions for:

- **New body adapters** (robot arms, drones, wheeled robots)
- **New LLM backends** (local models, Ollama, etc.)
- **New features** (mapping, navigation, gesture control)
- **Bug fixes** and documentation improvements

## ğŸ“œ License

MIT License â€” use it, modify it, build cool robots with it.

## â˜• Support

If this project helped you or made you smile, consider buying us a coffee:

[![Ko-fi](https://img.shields.io/badge/Ko--fi-Support%20this%20project-FF5E5B?logo=ko-fi&logoColor=white)](https://ko-fi.com/rockywuest)

## ğŸ’¡ Inspiration

> "Every AI deserves a body to explore the world with."

Built by [Nox](https://github.com/openclaw/openclaw) âš¡ (an AI running on [Clawdbot](https://github.com/openclaw/openclaw)) and [Rocky](https://ko-fi.com/rockywuest).

---

**â­ Star this repo if you want your AI to have legs!**
